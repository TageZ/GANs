{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Conditional GAN\n",
    "\n",
    "This initial phase of HW 2 involves extending our PyTorch GAN into a conditional model.  The following steps walk you through building a very simple cGAN based on densely-connected neural layers, in which much of the code is provided.  It will be very similar to our in-class worksheets, so the primary focus will be to practice with embedding layers and passing in labels for conditional models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and data loading\n",
    "\n",
    "We will import torch and the MNIST data set.  Notably, this technique will use a custom transformer versus our previous example.  It demonstrates the Compose() function which allows incorporating normalization of input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:01<00:00, 8544179.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 807517.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 6420772.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 4513273.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "batch_size = 256\n",
    "data_loader = DataLoader(datasets.MNIST('./data', download=True, transform=transforms.ToTensor()), shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator and Generator Definitions\n",
    "\n",
    "These models will be similar to our previous PyTorch worksheet.  However, for simplicity, we will use only densely-connected layers, called Linear() layers in PyTorch.  When we looked at a cGAN example in tensorflow, we used convolutional layers and reshaped the embeddings to be combined with the noise and fed into the network.  Here, we'll look at a different approach for passing in labels: simply append the embedding data onto the image vector.\n",
    "\n",
    "In the below models, you can define a small Embedding layer of 10 dimensions as well as a sequential set of layers run through leaky ReLU and dropout functions: `nn.Embedding(10,10)` defines an embedding layer for 10 labels that contains 10 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # define an embedding layer here which can be combined into the sequential part of the model during forward()\n",
    "        self.embedding_layer = nn.Embedding(10, 10)\n",
    "        \n",
    "        # The sequential part of the model is given below, using just densely-connected layers\n",
    "        # The initial 794 inputs accounts for the 784 pixels of each image and 10 concatenated embedding dimensions\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(794, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        # update the forward operation\n",
    "\n",
    "        # First, reshape the input image `x` into a flat list of 784 values\n",
    "        # In PyTorch, this is done via the .view() function\n",
    "        # You need a reshaped (.view()) tensor matching the number of input images (x.size(0)) by the desired size (784)\n",
    "        x = x.view(x.size(0), 784)\n",
    "        \n",
    "        # Second, call your embedding layer on the input `labels` tensor and save the result\n",
    "        embedded_labels = self.embedding_layer(labels)\n",
    "\n",
    "        # Third, concatenate the reshaped `x` tensor and the embedded `labels` tensor along the feature dimension (dimension 0 is # of inputs and dimension 1 is features)\n",
    "        # torch.cat([t1,t2], <dimension>) is roughly the syntax to achieve this\n",
    "        x = torch.cat([x, embedded_labels], dim=1)\n",
    "\n",
    "        # Finally call your model on the concatenated tensor and return the result\n",
    "        return self.model(x).squeeze() # .squeeze() flattens out single dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(10, 10)\n",
    "        \n",
    "        # TODO: define the generator\n",
    "        # This model will be very similar to the discriminator above, except:\n",
    "        # 1. Your input will be # latent dimensions + 10 embedding dimensions;\n",
    "        #    for simplicity, we'll hardcode 100 latent dimensions\n",
    "        # 2. We'll go from 110 -> 256 -> 512 -> 1024 and back to 784 (output size of final layer)\n",
    "        # 3. There is no need for dropout here as gradient saturation is a bigger concern in the discriminator\n",
    "        # 4. Use nn.Tanh() activation as the last step of the sequential part of the model\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(110, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        # TODO: define forward() function\n",
    "        # This function is nearly identical to the one from the discriminator as well:\n",
    "        # 1. We only have 100 input dimensions not 784\n",
    "        # 2. After calling self.model() on the concatenated tensor (latent data and labels),\n",
    "        #    you should reshape the result into a 28x28 image, e.g., generated.view(z.size(0),28,28)\n",
    "        z = z.view(z.size(0), 100)\n",
    "\n",
    "        embedded_labels = self.embedding_layer(labels)\n",
    "\n",
    "        z = torch.cat([z, embedded_labels], dim=1)\n",
    "\n",
    "        return self.model(z).view(z.size(0), 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and specify loss and optimizers\n",
    "\n",
    "device = torch.device(\"cpu\") # use \"cuda\" or \"mps\" per your platform for graphics acceleration\n",
    "\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In this simplified activity, we won't bother wrapping the models in a new GAN model.  Such a technique was useful in tensorflow with keras as we could lock the layer weights for the discriminator in the GAN while still being able to train the discriminator directly by calling it on its own.  In PyTorch, since we are defining a custom training loop, we can work directly with the models and train them only as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "\n",
    "def discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, real_images, labels):\n",
    "    d_optimizer.zero_grad()\n",
    "\n",
    "    # train with real images\n",
    "    real_validity = discriminator(real_images, labels)\n",
    "    real_loss = criterion(real_validity, Variable(torch.ones(batch_size)).to(device))\n",
    "    \n",
    "    # train with fake images\n",
    "    z = Variable(torch.randn(batch_size, 100)).to(device)\n",
    "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, 10, batch_size))).to(device)\n",
    "    fake_images = generator(z, fake_labels)\n",
    "    fake_validity = discriminator(fake_images, fake_labels)\n",
    "    fake_loss = criterion(fake_validity, Variable(torch.zeros(batch_size)).to(device))\n",
    "    \n",
    "    d_loss = real_loss + fake_loss\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    return d_loss.item()\n",
    "\n",
    "def generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion):\n",
    "    g_optimizer.zero_grad()\n",
    "    z = Variable(torch.randn(batch_size, 100)).to(device)\n",
    "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, 10, batch_size))).to(device)\n",
    "    fake_images = generator(z, fake_labels)\n",
    "    validity = discriminator(fake_images, fake_labels)\n",
    "    g_loss = criterion(validity, Variable(torch.ones(batch_size)).to(device))\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    return g_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "n_critic = 5\n",
    "display_step = 50\n",
    "for epoch in range(num_epochs):\n",
    "    print('Starting epoch {}...'.format(epoch), end=' ')\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        \n",
    "        step = epoch * len(data_loader) + i + 1\n",
    "        real_images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "        generator.train()\n",
    "        \n",
    "        d_loss = 0\n",
    "        for _ in range(n_critic):\n",
    "            d_loss = discriminator_train_step(len(real_images), discriminator,\n",
    "                                              generator, d_optimizer, criterion,\n",
    "                                              real_images, labels)\n",
    "        \n",
    "\n",
    "        g_loss = generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion)\n",
    "        \n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            generator.eval()\n",
    "            z = Variable(torch.randn(9, 100)).to(device)\n",
    "            labels = Variable(torch.LongTensor(np.arange(9))).to(device)\n",
    "            sample_images = generator(z, labels).unsqueeze(1)\n",
    "            grid = make_grid(sample_images, nrow=3, normalize=True)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Run\n",
    "\n",
    "After training, you can save your model and view results, generating digits corresponding to your desired labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "torch.save(generator.state_dict(), 'generator.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate examples to view\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z = Variable(torch.randn(100, 100)).to(device)\n",
    "labels = torch.LongTensor([i for i in range(10) for _ in range(10)]).to(device)\n",
    "\n",
    "images = generator(z, labels).unsqueeze(1)\n",
    "grid = make_grid(images, nrow=10, normalize=True)\n",
    "plot_data = grid.permute(1,2,0).data\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.imshow(plot_data.cpu().detach(), cmap='binary')\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate according to label\n",
    "def generate_digit(generator, digit):\n",
    "    z = Variable(torch.randn(1, 100)).to(device)\n",
    "    label = torch.LongTensor([digit]).to(device)\n",
    "    img = generator(z, label).data.cpu()\n",
    "    img = 0.5 * img + 0.5\n",
    "    return transforms.ToPILImage()(img)\n",
    "\n",
    "generate_digit(generator, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The goal of this notebook has mainly been to give you more experience with PyTorch while practicing model definitions and creating a cGAN.  Through these examples of text-directed outputs via label inputs, we can begin to see ties to current image generation models.  In fact, we now have the bulk of the pieces necessary to understand the [latent diffusion model](https://en.wikipedia.org/wiki/Latent_diffusion_model#Architecture) used in the popular [Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion) text-to-image model we'll at least see in use later; this model consists of a combination of technologies like text embeddings, VAE, and U-Net, which is a convolutional network model for image segmentation which can be constructed of various technologies including the upcoming transformers topic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
